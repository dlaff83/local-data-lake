{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from minio import Minio\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import SparkSession, functions as F, types\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Minio(\n",
    "    'localhost:9000',\n",
    "    access_key=os.getenv('DEV_USER_ACCESS_KEY'),\n",
    "    secret_key=os.getenv('DEV_USER_SECRET_KEY'),\n",
    "    secure=False\n",
    ")\n",
    "\n",
    "bucket_name = 'local-data-lake-bronze'\n",
    "\n",
    "found = client.bucket_exists(bucket_name)\n",
    "if not found:\n",
    "    client.make_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/30 21:35:58 WARN Utils: Your hostname, Denniss-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.6 instead (on interface en0)\n",
      "24/05/30 21:35:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/05/30 21:35:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector(spark://192.168.0.6:59902/jars/aws-java-sdk-bundle-1.12.262.jar, spark://192.168.0.6:59902/jars/hadoop-aws-3.3.4.jar)\n",
      "The Pyspark version 3.3.4 is running...\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master('local[5]') \\\n",
    "    .appName('local-data-lake') \\\n",
    "    .config('spark.jars', '../jars/hadoop-aws-3.3.4.jar,../jars/aws-java-sdk-bundle-1.12.262.jar') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "print(spark.sparkContext._jsc.sc().listJars())\n",
    "print(f'The Pyspark version {spark.version} is running...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc._jsc.hadoopConfiguration().set('fs.s3a.access.key', os.getenv('DEV_USER_ACCESS_KEY'))\n",
    "sc._jsc.hadoopConfiguration().set('fs.s3a.secret.key', os.getenv('DEV_USER_SECRET_KEY'))\n",
    "sc._jsc.hadoopConfiguration().set('fs.s3a.endpoint', 'http://127.0.0.1:9000')\n",
    "sc._jsc.hadoopConfiguration().set('fs.s3a.path.style.access', 'true')\n",
    "sc._jsc.hadoopConfiguration().set('fs.s3a.connection.ssl.enabled', 'false')\n",
    "sc._jsc.hadoopConfiguration().set('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "sc._jsc.hadoopConfiguration().set('fs.s3a.connection.ssl.enabled', 'false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_data(dt):\n",
    "    url = f'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{dt}.parquet'\n",
    "    sc.addFile(url)\n",
    "    \n",
    "    df = spark.read.parquet(SparkFiles.get(f'yellow_tripdata_{dt}.parquet'))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/30 21:36:25 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dt = '2024-03'\n",
    "\n",
    "df = load_taxi_data(dt)\n",
    "df.count()\n",
    "\n",
    "df \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet(f's3a://local-data-lake-bronze/{dt}/yellow_taxi.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
