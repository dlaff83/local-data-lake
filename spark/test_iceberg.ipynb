{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import SparkSession, functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_S3_ENDPOINT = os.environ.get('AWS_S3_ENDPOINT')\n",
    "AWS_ACCESS_KEY_ID = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "AWS_SECRET_ACCESS_KEY = os.environ.get('AWS_SECRET_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .set('spark.jars.packages','org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,org.apache.iceberg:iceberg-aws-bundle:1.5.2,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262')\n",
    "        .set('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "        .set('spark.hadoop.fs.s3a.path.style.access', 'true')\n",
    "        .set('spark.hadoop.fs.s3a.connection.ssl.enabled', 'false')\n",
    "        .set('spark.hadoop.fs.s3a.endpoint', AWS_S3_ENDPOINT)\n",
    "        .set('spark.hadoop.fs.s3a.access.key', AWS_ACCESS_KEY_ID)\n",
    "        .set('spark.hadoop.fs.s3a.secret.key', AWS_SECRET_ACCESS_KEY)        \n",
    "        .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions') # Use Iceberg with Spark\n",
    "        .set('spark.sql.catalog.silver', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.silver.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n",
    "        .set('spark.sql.catalog.silver.warehouse', 's3a://local-data-lake-silver/')\n",
    "        .set('spark.sql.catalog.silver.s3.endpoint', AWS_S3_ENDPOINT)\n",
    "        .set('spark.sql.defaultCatalog', 'silver') # Name of the Iceberg catalog\n",
    "        .set('spark.sql.catalogImplementation', 'in-memory')\n",
    "        .set('spark.sql.catalog.silver.type', 'hadoop') # Iceberg catalog type\n",
    "        .set('spark.executor.heartbeatInterval', '300000')\n",
    "        .set('spark.network.timeout', '400000')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/dennislafferty/.pyenv/versions/3.10.13/envs/data-platform/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/dennislafferty/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/dennislafferty/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.apache.iceberg#iceberg-aws-bundle added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-44d15b67-27c9-43c2-bcb1-f46fdae48616;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2 in central\n",
      "\tfound org.apache.iceberg#iceberg-aws-bundle;1.5.2 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 187ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-aws-bundle;1.5.2 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-44d15b67-27c9-43c2-bcb1-f46fdae48616\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/7ms)\n",
      "24/06/12 21:18:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Pyspark version 3.5.1 is running...\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master('local[5]') \\\n",
    "    .appName('local-data-lake-iceberg') \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "print(f'The Pyspark version {spark.version} is running...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reads data from bronze bucket based on dt\n",
    "dt = '2023-02'\n",
    "df = spark.read.parquet(f's3a://local-data-lake-bronze/{dt}/yellow_taxi.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2913955"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creates a new df called df_dt with a new column with a constant of dt\n",
    "df_dt = df.withColumn('dt', F.lit(dt))\n",
    "df_dt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#writes to the new catalog table and partitions by dt\n",
    "# df_dt.writeTo('silver.nyc.taxi') \\\n",
    "#     .partitionedBy('dt') \\\n",
    "#     .createOrReplace() \n",
    "\n",
    "#appends data\n",
    "df_dt.writeTo('silver.nyc.taxi') \\\n",
    "    .partitionedBy('dt') \\\n",
    "    .append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|row_count|\n",
      "+---------+\n",
      "|  3066766|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT COUNT(1) as row_count FROM silver.nyc.taxi').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|row_count|\n",
      "+---------+\n",
      "|  5980721|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT COUNT(1) as row_count FROM silver.nyc.taxi').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|row_count|\n",
      "+---------+\n",
      "|  3066766|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "        SELECT COUNT(1) as row_count \n",
    "        FROM silver.nyc.taxi\n",
    "        WHERE dt = '2023-01'\n",
    "    ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "|partition|spec_id|record_count|file_count|total_data_file_size_in_bytes|position_delete_record_count|position_delete_file_count|equality_delete_record_count|equality_delete_file_count|     last_updated_at|last_updated_snapshot_id|\n",
      "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "|{2023-02}|      0|     2913955|         1|                     45522430|                           0|                         0|                           0|                         0|2024-06-12 21:26:...|     9112964658152648116|\n",
      "|{2023-01}|      0|     3066766|         1|                     47732041|                           0|                         0|                           0|                         0|2024-06-12 21:19:...|     8117842573201501966|\n",
      "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "        SELECT * FROM silver.nyc.taxi.partitions;\n",
    "    ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------+-------+---------+------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+-------------+------------+-------------+--------------------+\n",
      "|content|           file_path|file_format|spec_id|partition|record_count|file_size_in_bytes|        column_sizes|        value_counts|   null_value_counts|    nan_value_counts|        lower_bounds|        upper_bounds|key_metadata|split_offsets|equality_ids|sort_order_id|    readable_metrics|\n",
      "+-------+--------------------+-----------+-------+---------+------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+-------------+------------+-------------+--------------------+\n",
      "|      0|s3a://local-data-...|    PARQUET|      0|{2023-02}|     2913955|          45522430|{1 -> 369127, 2 -...|{1 -> 2913955, 2 ...|{1 -> 0, 2 -> 0, ...|{4 -> 0, 5 -> 0, ...|{1 -> [01 00 00 0...|{1 -> [06 00 00 0...|        NULL|          [4]|        NULL|            0|{{2863861, 291395...|\n",
      "|      0|s3a://local-data-...|    PARQUET|      0|{2023-01}|     3066766|          47732041|{1 -> 374016, 2 -...|{1 -> 3066766, 2 ...|{1 -> 0, 2 -> 0, ...|{4 -> 0, 5 -> 0, ...|{1 -> [01 00 00 0...|{1 -> [02 00 00 0...|        NULL|          [4]|        NULL|            0|{{3067101, 306676...|\n",
      "+-------+--------------------+-----------+-------+---------+------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+-------------+------------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "        SELECT * FROM silver.nyc.taxi.files;\n",
    "    ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------+-------+---------+------------+------------------+\n",
      "|content|           file_path|file_format|spec_id|partition|record_count|file_size_in_bytes|\n",
      "+-------+--------------------+-----------+-------+---------+------------+------------------+\n",
      "|      0|s3a://local-data-...|    PARQUET|      0|{2023-02}|     2913955|          45522430|\n",
      "|      0|s3a://local-data-...|    PARQUET|      0|{2023-01}|     3066766|          47732041|\n",
      "+-------+--------------------+-----------+-------+---------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "        SELECT content\n",
    "         , file_path\n",
    "         , file_format\n",
    "         , spec_id\n",
    "         , partition\n",
    "         , record_count\n",
    "         , file_size_in_bytes\n",
    "        FROM silver.nyc.taxi.files;\n",
    "    ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-platform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
