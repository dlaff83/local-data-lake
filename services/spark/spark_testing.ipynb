{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from minio import Minio\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_S3_ENDPOINT = os.environ.get('AWS_S3_ENDPOINT')\n",
    "AWS_ACCESS_KEY_ID = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "AWS_SECRET_ACCESS_KEY = os.environ.get('AWS_SECRET_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bucket(bucket_name):\n",
    "    client = Minio(\n",
    "        'localhost:9000',\n",
    "        access_key=AWS_ACCESS_KEY_ID,\n",
    "        secret_key=AWS_SECRET_ACCESS_KEY,\n",
    "        secure=False\n",
    "    )\n",
    "\n",
    "    found = client.bucket_exists(bucket_name)\n",
    "    if not found:\n",
    "        client.make_bucket(bucket_name)\n",
    "        return print(f'Creating bucket: {bucket_name}')\n",
    "    else:\n",
    "        return print(f'Bucket: {bucket_name} already in use')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bronze bucket \n",
    "create_bucket('local-data-lake-bronze')\n",
    "\n",
    "#Silver bucket\n",
    "create_bucket('local-data-lake-silver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .set('spark.jars.packages','org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262')\n",
    "        .set('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "        .set('spark.hadoop.fs.s3a.path.style.access', 'true')\n",
    "        .set('spark.hadoop.fs.s3a.connection.ssl.enabled', 'false')\n",
    "        .set('spark.hadoop.fs.s3a.endpoint', AWS_S3_ENDPOINT)\n",
    "        .set('spark.hadoop.fs.s3a.access.key', AWS_ACCESS_KEY_ID)\n",
    "        .set('spark.hadoop.fs.s3a.secret.key', AWS_SECRET_ACCESS_KEY)      \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master('local[5]') \\\n",
    "    .appName('local-data-platform') \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "print(f'The Pyspark version {spark.version} is running...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_data(dt):\n",
    "    url = f'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{dt}.parquet'\n",
    "    sc.addFile(url)\n",
    "    \n",
    "    df = spark.read.parquet(SparkFiles.get(f'yellow_tripdata_{dt}.parquet'))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = '2023-06'\n",
    "\n",
    "df = load_taxi_data(dt)\n",
    "print(f'Total number of file rows: {df.count()}')\n",
    "\n",
    "df \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet(f's3a://local-data-lake-bronze/{dt}/yellow_taxi.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
